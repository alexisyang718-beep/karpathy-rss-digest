#!/usr/bin/env python3
"""
Karpathy RSS Daily Digest
åŸºäº Andrej Karpathy æ¨èçš„ 92 ä¸ªé¡¶çº§ç§‘æŠ€åšå®¢ RSS æºï¼Œ
æ¯å¤©è‡ªåŠ¨æŠ“å–æ–‡ç« å…¨æ–‡ï¼Œç”¨ AI ç”Ÿæˆé«˜è´¨é‡ä¸­æ–‡æ ‡é¢˜ã€æ‘˜è¦å’Œè¯¦ç»†è§£è¯»ï¼Œ
ç”Ÿæˆå¯å…¬å¼€è®¿é—®çš„ç½‘é¡µï¼ˆGitHub Pagesï¼‰ï¼Œå¹¶æ¨é€ç²¾é€‰åˆ°ä¼ä¸šå¾®ä¿¡ç¾¤ã€‚

ç”¨æ³•:
    python rss_reader.py                          # æŠ“å–å¹¶ç”Ÿæˆä»Šæ—¥ç²¾é€‰
    python rss_reader.py --days 3                 # æŠ“å–æœ€è¿‘3å¤©çš„å†…å®¹
    python rss_reader.py --webhook <URL>          # æŠ“å–å¹¶æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡ç¾¤
    python rss_reader.py --watch --webhook <URL>  # å®æ—¶ç›‘æ§æ¨¡å¼ï¼Œå‘ç°æ–°æ–‡ç« è‡ªåŠ¨æ¨é€
    python rss_reader.py --watch --interval 15    # æ¯15åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼ˆé»˜è®¤30åˆ†é’Ÿï¼‰
    python rss_reader.py --schedule               # æ¯å¤©æ—©ä¸Š8ç‚¹ç”Ÿæˆæ—¥æŠ¥
"""

import asyncio
import argparse
import hashlib
import html as html_mod
import json
import logging
import os
import re
import sys
import xml.etree.ElementTree as ET
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional

import feedparser
import httpx
from bs4 import BeautifulSoup
from dateutil import parser as date_parser
from jinja2 import Template
from openai import OpenAI

# â”€â”€ æ—¥å¿—é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# â”€â”€ å¸¸é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_CONCURRENT = 20
MAX_FETCH_PAGE = 10
REQUEST_TIMEOUT = 15.0
PAGE_TIMEOUT = 20.0
MAX_ARTICLES_NO_DATE = 3
MAX_CONTENT_LEN = 2000
LLM_BATCH_SIZE = 5
DEFAULT_WATCH_INTERVAL = 30
WECOM_MSG_MAX_LEN = 4096
BASE_DIR = Path(__file__).parent
OUTPUT_DIR = BASE_DIR / "output"
DOCS_DIR = BASE_DIR / "docs"          # GitHub Pages ç›®å½•
FEEDS_FILE = BASE_DIR / "feeds.opml"
SENT_DB_FILE = OUTPUT_DIR / ".sent_articles.json"

# å†…å®¹ç­›é€‰é…ç½®ï¼ˆé»˜è®¤åªä¿ç•™ç§‘æŠ€/AI/å•†ä¸šç›¸å…³å†…å®¹ï¼‰
ENABLE_CONTENT_FILTER = os.environ.get("ENABLE_CONTENT_FILTER", "true").lower() != "false"

# DeepSeek API é…ç½®
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
DEEPSEEK_BASE_URL = "https://api.deepseek.com"
DEEPSEEK_MODEL = "deepseek-chat"

# GitHub Pages é…ç½®ï¼ˆæ¨é€åè‡ªåŠ¨ç”Ÿæˆï¼‰
GITHUB_PAGES_URL = os.environ.get("GITHUB_PAGES_URL", "https://ä½ çš„ç”¨æˆ·å.github.io/karpathy-rss-digest")


# â”€â”€ æ•°æ®æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class FeedSource:
    name: str
    xml_url: str
    html_url: str


@dataclass
class Article:
    title: str
    link: str
    source: str
    published: Optional[datetime] = None
    summary: str = ""
    author: str = ""
    tags: list = field(default_factory=list)
    full_content: str = ""
    ai_title: str = ""
    ai_summary: str = ""        # ä¸€å¥è¯æ‘˜è¦ï¼ˆä¼å¾®æ¨é€ç”¨ï¼‰
    ai_detail: str = ""         # è¯¦ç»†ä¸­æ–‡è§£è¯»ï¼ˆç½‘é¡µå±•ç¤ºç”¨ï¼‰
    category: str = ""          # AIåˆ¤æ–­çš„ç±»åˆ«ï¼šç§‘æŠ€/AI/å•†ä¸š/å…¶ä»–
    is_relevant: bool = True    # æ˜¯å¦å±äºç§‘æŠ€/AI/å•†ä¸šç›¸å…³å†…å®¹


# â”€â”€ å·²æ¨é€æ–‡ç« å»é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _article_id(article: Article) -> str:
    return hashlib.md5(article.link.encode()).hexdigest()


def load_sent_db() -> dict:
    if SENT_DB_FILE.exists():
        try:
            data = json.loads(SENT_DB_FILE.read_text(encoding="utf-8"))
            cutoff = (datetime.now(timezone.utc) - timedelta(days=30)).isoformat()
            return {k: v for k, v in data.items() if v.get("sent_at", "") > cutoff}
        except Exception:
            return {}
    return {}


def save_sent_db(db: dict):
    SENT_DB_FILE.parent.mkdir(parents=True, exist_ok=True)
    SENT_DB_FILE.write_text(json.dumps(db, ensure_ascii=False, indent=2), encoding="utf-8")


def filter_new_articles(articles: list[Article], sent_db: dict) -> list[Article]:
    return [a for a in articles if _article_id(a) not in sent_db]


def mark_as_sent(articles: list[Article], sent_db: dict) -> dict:
    for a in articles:
        sent_db[_article_id(a)] = {
            "title": a.title, "link": a.link,
            "sent_at": datetime.now(timezone.utc).isoformat(),
        }
    return sent_db


# â”€â”€ OPML è§£æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_opml(filepath: Path) -> list[FeedSource]:
    tree = ET.parse(filepath)
    root = tree.getroot()
    feeds = []
    for outline in root.iter("outline"):
        xml_url = outline.get("xmlUrl")
        if xml_url:
            feeds.append(FeedSource(
                name=outline.get("text", outline.get("title", "Unknown")),
                xml_url=xml_url,
                html_url=outline.get("htmlUrl", ""),
            ))
    logger.info(f"ä» OPML ä¸­è§£æåˆ° {len(feeds)} ä¸ª RSS æº")
    return feeds


# â”€â”€ RSS æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_html(raw: str) -> str:
    text = re.sub(r"<[^>]+>", "", raw)
    text = html_mod.unescape(text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def parse_date(entry: dict) -> Optional[datetime]:
    for key in ("published", "updated", "created"):
        val = entry.get(key)
        if val:
            try:
                dt = date_parser.parse(val)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt
            except (ValueError, TypeError):
                continue
    for key in ("published_parsed", "updated_parsed", "created_parsed"):
        val = entry.get(key)
        if val:
            try:
                from time import mktime
                dt = datetime.fromtimestamp(mktime(val), tz=timezone.utc)
                return dt
            except (ValueError, TypeError, OverflowError):
                continue
    return None


async def fetch_feed(client: httpx.AsyncClient, source: FeedSource, since: datetime) -> list[Article]:
    articles = []
    try:
        resp = await client.get(source.xml_url, follow_redirects=True)
        resp.raise_for_status()
        feed = feedparser.parse(resp.text)
        has_any_date = any(parse_date(e) is not None for e in feed.entries)
        collected = 0
        for entry in feed.entries:
            pub_date = parse_date(entry)
            if pub_date and pub_date < since:
                continue
            if pub_date is None:
                if has_any_date:
                    continue
                else:
                    collected += 1
                    if collected > MAX_ARTICLES_NO_DATE:
                        continue
            content_raw = ""
            if entry.get("content"):
                content_raw = entry["content"][0].get("value", "")
            if not content_raw:
                content_raw = entry.get("summary", "") or entry.get("description", "") or ""
            summary = clean_html(content_raw)
            tags = [t.get("term", "") for t in entry.get("tags", []) if t.get("term")]
            articles.append(Article(
                title=entry.get("title", "æ— æ ‡é¢˜"),
                link=entry.get("link", source.html_url),
                source=source.name,
                published=pub_date,
                summary=summary[:500] if summary else "",
                author=entry.get("author", ""),
                tags=tags[:5],
                full_content=summary,
            ))
    except httpx.TimeoutException:
        logger.warning(f"â° è¶…æ—¶: {source.name} ({source.xml_url})")
    except httpx.HTTPStatusError as e:
        logger.warning(f"âŒ HTTP {e.response.status_code}: {source.name}")
    except Exception as e:
        logger.warning(f"âš ï¸  å¤±è´¥: {source.name} - {type(e).__name__}: {e}")
    return articles


async def fetch_all_feeds(feeds: list[FeedSource], since: datetime) -> list[Article]:
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)
    async def bounded_fetch(client, source):
        async with semaphore:
            return await fetch_feed(client, source, since)
    async with httpx.AsyncClient(
        timeout=httpx.Timeout(REQUEST_TIMEOUT),
        headers={"User-Agent": "KarpathyRSS-DailyDigest/1.0"},
        limits=httpx.Limits(max_connections=MAX_CONCURRENT, max_keepalive_connections=10),
    ) as client:
        tasks = [bounded_fetch(client, feed) for feed in feeds]
        results = await asyncio.gather(*tasks)
    all_articles = []
    for result in results:
        all_articles.extend(result)
    all_articles.sort(
        key=lambda a: a.published or datetime.min.replace(tzinfo=timezone.utc),
        reverse=True,
    )
    logger.info(f"å…±æŠ“å–åˆ° {len(all_articles)} ç¯‡æ–‡ç« ")
    return all_articles


# â”€â”€ ç½‘é¡µå…¨æ–‡æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_text_from_html(html_content: str) -> str:
    soup = BeautifulSoup(html_content, "html.parser")
    for tag in soup(["script", "style", "nav", "header", "footer", "aside",
                      "form", "iframe", "noscript", "svg", "img"]):
        tag.decompose()
    article = soup.find("article")
    if article:
        text = article.get_text(separator="\n", strip=True)
    else:
        for selector in [".post-content", ".entry-content", ".article-body",
                         ".content", "main", "#content", ".post"]:
            container = soup.select_one(selector)
            if container and len(container.get_text(strip=True)) > 200:
                text = container.get_text(separator="\n", strip=True)
                break
        else:
            text = soup.get_text(separator="\n", strip=True)
    lines = [line.strip() for line in text.split("\n") if line.strip()]
    return "\n".join(lines)


async def fetch_page_content(client: httpx.AsyncClient, article: Article) -> str:
    try:
        resp = await client.get(article.link, follow_redirects=True)
        resp.raise_for_status()
        text = extract_text_from_html(resp.text)
        if len(text) > 200:
            return text
    except Exception as e:
        logger.debug(f"ç½‘é¡µæŠ“å–å¤±è´¥ {article.link}: {e}")
    return ""


async def enrich_articles_with_full_content(articles: list[Article]):
    logger.info(f"ğŸ“„ å¼€å§‹æŠ“å– {len(articles)} ç¯‡æ–‡ç« å…¨æ–‡...")
    semaphore = asyncio.Semaphore(MAX_FETCH_PAGE)
    async def bounded_fetch(client, article):
        async with semaphore:
            content = await fetch_page_content(client, article)
            if content:
                article.full_content = content
    async with httpx.AsyncClient(
        timeout=httpx.Timeout(PAGE_TIMEOUT),
        headers={"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"},
        limits=httpx.Limits(max_connections=MAX_FETCH_PAGE, max_keepalive_connections=5),
    ) as client:
        tasks = [bounded_fetch(client, a) for a in articles]
        await asyncio.gather(*tasks)
    has_content = sum(1 for a in articles if len(a.full_content) > 200)
    logger.info(f"âœ… æˆåŠŸè·å– {has_content}/{len(articles)} ç¯‡æ–‡ç« å…¨æ–‡")


# â”€â”€ AI æ‘˜è¦ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def create_llm_client() -> OpenAI:
    return OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)


SUMMARIZE_PROMPT = """\
ä½ æ˜¯ç§‘æŠ€ç¼–è¾‘ã€‚åˆ¤æ–­æ–‡ç« ç±»åˆ«å¹¶ç”Ÿæˆä¸­æ–‡æ ‡é¢˜å’Œæ‘˜è¦ã€‚

ç±»åˆ«ï¼šAIï¼ˆäººå·¥æ™ºèƒ½/ML/LLMï¼‰ã€ç§‘æŠ€ï¼ˆå¼€å‘/äº‘è®¡ç®—/ç¡¬ä»¶/å®‰å…¨ï¼‰ã€å•†ä¸šï¼ˆç§‘æŠ€å…¬å¸/åˆ›ä¸š/æŠ•èµ„ï¼‰ã€å…¶ä»–ã€‚
éç§‘æŠ€ç±»ç›´æ¥è¿”å› is_relevant=falseï¼Œtitle/summary ç•™ç©ºã€‚

JSON æ ¼å¼ï¼ˆä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ï¼‰ï¼š
{"category": "AI/ç§‘æŠ€/å•†ä¸š/å…¶ä»–", "is_relevant": true/false, "title": "ä¸­æ–‡æ ‡é¢˜(â‰¤30å­—)", "summary": "ä¸€å¥è¯æ‘˜è¦(â‰¤80å­—)"}
"""

DETAIL_PROMPT = """\
ä½ æ˜¯èµ„æ·±ç§‘æŠ€ç¼–è¾‘ã€‚ç”¨5-8å¥è¯å†™å®Œæ•´ä¸­æ–‡è§£è¯»ï¼šç¬¬ä¸€æ®µè®²æ–‡ç« å†…å®¹ï¼Œç¬¬äºŒæ®µæç‚¼æ ¸å¿ƒè§‚ç‚¹/æ•°æ®ï¼Œç¬¬ä¸‰æ®µè¯´å¯¹ä»ä¸šè€…çš„å¯å‘ã€‚ä¸“æœ‰åè¯ä¿ç•™è‹±æ–‡ï¼ˆGPTã€Transformerã€Rustç­‰ï¼‰ã€‚åªè¾“å‡ºè§£è¯»æ–‡æœ¬ï¼Œä¸åŠ å…¶ä»–å†…å®¹ã€‚
"""


def summarize_with_llm(client: OpenAI, articles: list[Article]) -> list[dict]:
    results = []
    for article in articles:
        content = article.full_content or article.summary or ""
        if not content:
            results.append({"title": article.title, "summary": "", "category": "å…¶ä»–", "is_relevant": False})
            continue
        content_trimmed = content[:MAX_CONTENT_LEN]
        user_msg = f"åŸæ ‡é¢˜: {article.title}\næ¥æº: {article.source}\n\n{content_trimmed}"
        try:
            response = client.chat.completions.create(
                model=DEEPSEEK_MODEL,
                messages=[
                    {"role": "system", "content": SUMMARIZE_PROMPT},
                    {"role": "user", "content": user_msg},
                ],
                temperature=0.3,
                max_tokens=200,
            )
            resp_text = response.choices[0].message.content.strip()
            json_match = re.search(r'\{.*\}', resp_text, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                is_relevant = data.get("is_relevant", True)
                category = data.get("category", "å…¶ä»–")
                if category == "å…¶ä»–":
                    is_relevant = False
                results.append({
                    "title": data.get("title", article.title) if is_relevant else article.title,
                    "summary": data.get("summary", "") if is_relevant else "",
                    "category": category,
                    "is_relevant": is_relevant,
                })
            else:
                results.append({"title": article.title, "summary": "", "category": "å…¶ä»–", "is_relevant": False})
        except Exception as e:
            logger.warning(f"LLM æ‘˜è¦å¤±è´¥ [{article.title[:30]}]: {e}")
            results.append({"title": article.title, "summary": article.summary, "category": "å…¶ä»–", "is_relevant": False})
    return results


def enrich_detail_with_llm(client: OpenAI, articles: list[Article]) -> None:
    """å¯¹å·²è¿‡æ»¤çš„ç›¸å…³æ–‡ç« è¡¥å……è¯¦ç»†ä¸­æ–‡è§£è¯»ï¼ˆç½‘é¡µå±•ç¤ºç”¨ï¼‰"""
    for article in articles:
        content = article.full_content or article.summary or ""
        if not content:
            article.ai_detail = article.ai_summary
            continue
        content_trimmed = content[:MAX_CONTENT_LEN]
        user_msg = f"æ ‡é¢˜: {article.ai_title or article.title}\næ¥æº: {article.source}\n\n{content_trimmed}"
        try:
            response = client.chat.completions.create(
                model=DEEPSEEK_MODEL,
                messages=[
                    {"role": "system", "content": DETAIL_PROMPT},
                    {"role": "user", "content": user_msg},
                ],
                temperature=0.5,
                max_tokens=400,
            )
            article.ai_detail = response.choices[0].message.content.strip()
        except Exception as e:
            logger.warning(f"LLM è¯¦ç»†è§£è¯»å¤±è´¥ [{article.title[:30]}]: {e}")
            article.ai_detail = article.ai_summary


def ai_summarize_articles(articles: list[Article], enable_filter: bool = True) -> list[Article]:
    if not articles:
        return articles
    logger.info(f"ğŸ§  å¼€å§‹ç”¨ AI ç”Ÿæˆ {len(articles)} ç¯‡æ–‡ç« çš„ä¸­æ–‡è§£è¯»...")
    if enable_filter:
        logger.info("   ğŸ“Œ å†…å®¹ç­›é€‰å·²å¯ç”¨ï¼šåªä¿ç•™ç§‘æŠ€/AI/å•†ä¸šç›¸å…³å†…å®¹")
    client = create_llm_client()
    total = len(articles)
    for i in range(0, total, LLM_BATCH_SIZE):
        batch = articles[i:i + LLM_BATCH_SIZE]
        batch_num = i // LLM_BATCH_SIZE + 1
        total_batches = (total + LLM_BATCH_SIZE - 1) // LLM_BATCH_SIZE
        logger.info(f"  å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ç¯‡)")
        results = summarize_with_llm(client, batch)
        for j, result in enumerate(results):
            idx = i + j
            articles[idx].ai_title = result["title"]
            articles[idx].ai_summary = result["summary"]
            articles[idx].category = result["category"]
            articles[idx].is_relevant = result["is_relevant"]

    if enable_filter:
        relevant_articles = [a for a in articles if a.is_relevant]
        filtered_out = len(articles) - len(relevant_articles)
        if filtered_out > 0:
            logger.info(f"âœ… åˆ†ç±»å®Œæˆ: ä¿ç•™ {len(relevant_articles)} ç¯‡ç›¸å…³æ–‡ç« , è¿‡æ»¤æ‰ {filtered_out} ç¯‡éç§‘æŠ€/AI/å•†ä¸šå†…å®¹")
        else:
            logger.info(f"âœ… åˆ†ç±»å®Œæˆ: å…¨éƒ¨ {len(relevant_articles)} ç¯‡æ–‡ç« å‡ä¸ºç›¸å…³å†…å®¹")
    else:
        relevant_articles = articles
        logger.info(f"âœ… åˆ†ç±»å®Œæˆ: {len(articles)} ç¯‡æ–‡ç« ")

    # ä»…å¯¹ç›¸å…³æ–‡ç« ç”Ÿæˆè¯¦ç»†è§£è¯»ï¼ˆç½‘é¡µå±•ç¤ºç”¨ï¼‰
    if relevant_articles:
        logger.info(f"ğŸ§  ç”Ÿæˆ {len(relevant_articles)} ç¯‡ç›¸å…³æ–‡ç« çš„è¯¦ç»†ä¸­æ–‡è§£è¯»...")
        enrich_detail_with_llm(client, relevant_articles)
        logger.info(f"âœ… è¯¦ç»†è§£è¯»ç”Ÿæˆå®Œæˆ")

    return relevant_articles


# â”€â”€ ä¼ä¸šå¾®ä¿¡æ¨é€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _utf8_len(text: str) -> int:
    """è®¡ç®—å­—ç¬¦ä¸²çš„ UTF-8 å­—èŠ‚é•¿åº¦ï¼ˆä¼ä¸šå¾®ä¿¡æŒ‰å­—èŠ‚é™åˆ¶ï¼‰"""
    return len(text.encode("utf-8"))


def _select_top_articles(articles: list[Article], n: int = 5) -> list[Article]:
    """ä»æ‰€æœ‰æ–‡ç« ä¸­æŒ‘é€‰æœ€é‡è¦çš„ n ç¯‡ï¼šAI ç±»ä¼˜å…ˆï¼Œå…¶æ¬¡ç§‘æŠ€ï¼Œå…¶æ¬¡å•†ä¸šï¼ŒåŒç±»æŒ‰å‘å¸ƒæ—¶é—´é™åº"""
    priority = {"AI": 0, "ç§‘æŠ€": 1, "å•†ä¸š": 2}
    sorted_articles = sorted(
        articles,
        key=lambda a: (
            priority.get(a.category, 3),
            -(a.published.timestamp() if a.published else 0),
        ),
    )
    return sorted_articles[:n]


def _build_wecom_markdown(articles: list[Article], page_url: str = "", total_count: int = 0) -> str:
    """æ„å»ºä¼ä¸šå¾®ä¿¡ Markdown æ¶ˆæ¯ï¼šç²¾ç®€æ‘˜è¦ + åº•éƒ¨å®Œæ•´è§£è¯»é“¾æ¥"""
    header = f"ğŸ“¡ **Karpathy RSS ç²¾é€‰**\n> {datetime.now().strftime('%Y-%m-%d %H:%M')}"
    if total_count > len(articles):
        header += f"  |  æœ¬æœŸ {total_count} ç¯‡ï¼Œç²¾é€‰ {len(articles)} ç¯‡"
    header += "\n\n"

    body = ""
    for i, a in enumerate(articles):
        title = a.ai_title or a.title
        summary = a.ai_summary or ""
        time_str = a.published.strftime('%m-%d %H:%M') if a.published else "è¿‘æœŸ"

        block = f"**{i + 1}. {title}**\n"
        block += f"> {a.source} Â· {time_str}\n"
        if summary:
            if len(summary) > 80:
                summary = summary[:80] + "..."
            block += f"> {summary}\n"
        block += "\n"
        body += block

    footer = ""
    if page_url:
        footer = f"> [ğŸ‘‰ æŸ¥çœ‹å…¨éƒ¨ {total_count} ç¯‡å®Œæ•´ä¸­æ–‡è§£è¯»]({page_url})" if total_count > len(articles) else f"> [ğŸ‘‰ æŸ¥çœ‹å®Œæ•´ä¸­æ–‡è§£è¯»]({page_url})"

    return header + body + footer


async def send_to_wecom(webhook_url: str, articles: list[Article], page_url: str = ""):
    if not articles:
        logger.info("æ²¡æœ‰æ–°æ–‡ç« éœ€è¦æ¨é€")
        return
    total_count = len(articles)
    top_articles = _select_top_articles(articles, n=5)
    msg = _build_wecom_markdown(top_articles, page_url, total_count=total_count)
    logger.info(f"ğŸ“¤ å‘ä¼ä¸šå¾®ä¿¡æ¨é€ç²¾é€‰ {len(top_articles)}/{total_count} ç¯‡æ–‡ç« ")
    async with httpx.AsyncClient(timeout=httpx.Timeout(15.0)) as client:
        payload = {"msgtype": "markdown", "markdown": {"content": msg}}
        try:
            resp = await client.post(webhook_url, json=payload)
            resp.raise_for_status()
            result = resp.json()
            if result.get("errcode") == 0:
                logger.info(f"  âœ… æ¶ˆæ¯å‘é€æˆåŠŸ")
            else:
                logger.warning(f"  âŒ æ¶ˆæ¯å‘é€å¤±è´¥: {result}")
        except Exception as e:
            logger.error(f"  âŒ æ¶ˆæ¯å‘é€å¼‚å¸¸: {e}")


# â”€â”€ åˆ†ç±» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def categorize_articles(articles: list[Article]) -> dict[str, list[Article]]:
    """ä½¿ç”¨AIåˆ¤æ–­çš„categoryè¿›è¡Œåˆ†ç±»"""
    # å®šä¹‰ç±»åˆ«åˆ°å±•ç¤ºåç§°çš„æ˜ å°„
    category_mapping = {
        "AI": "ğŸ¤– AI / æœºå™¨å­¦ä¹ ",
        "ç§‘æŠ€": "ğŸ’» ç§‘æŠ€ / æŠ€æœ¯",
        "å•†ä¸š": "ğŸ“ˆ å•†ä¸š / è¡Œä¸š",
    }
    
    categories = {
        "ğŸ¤– AI / æœºå™¨å­¦ä¹ ": [],
        "ğŸ’» ç§‘æŠ€ / æŠ€æœ¯": [],
        "ğŸ“ˆ å•†ä¸š / è¡Œä¸š": [],
    }
    
    for a in articles:
        cat = a.category or "å…¶ä»–"
        display_name = category_mapping.get(cat)
        if display_name and display_name in categories:
            categories[display_name].append(a)
    
    # ç§»é™¤ç©ºåˆ†ç±»
    return {k: v for k, v in categories.items() if v}


# â”€â”€ é¡µé¢ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HTML_TEMPLATE = Template("""\
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Karpathy RSS å®æ—¶ç²¾é€‰ - {{ date }}</title>
<style>
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Noto Sans SC', sans-serif; background: #0a0a0a; color: #e0e0e0; line-height: 1.8; }
  .container { max-width: 860px; margin: 0 auto; padding: 30px 20px; }
  h1 { font-size: 1.6em; margin-bottom: 6px; color: #fff; }
  .site-desc { color: #666; font-size: 0.85em; margin-bottom: 20px; }
  .meta { color: #888; font-size: 0.88em; margin-bottom: 30px; border-bottom: 1px solid #222; padding-bottom: 15px; }
  .category { margin-bottom: 35px; }
  .category h2 { font-size: 1.2em; color: #4fc3f7; margin-bottom: 15px; padding-bottom: 8px; border-bottom: 1px solid #1a1a1a; }
  .article { background: #111; border-radius: 12px; padding: 22px 24px; margin-bottom: 16px; border: 1px solid #1e1e1e; transition: border-color 0.2s, transform 0.1s; }
  .article:hover { border-color: #333; transform: translateY(-1px); }
  .article h3 { font-size: 1.08em; margin-bottom: 8px; color: #fff; line-height: 1.5; }
  .article-meta { font-size: 0.8em; color: #666; margin-bottom: 12px; display: flex; gap: 16px; flex-wrap: wrap; align-items: center; }
  .article-meta .category-tag { background: #1a3a2a; color: #4ade80; padding: 2px 8px; border-radius: 4px; font-size: 0.85em; }
  .detail { font-size: 0.93em; color: #bbb; line-height: 1.9; margin-bottom: 14px; white-space: pre-line; }
  .read-original { display: inline-block; font-size: 0.85em; color: #4fc3f7; text-decoration: none; padding: 6px 16px; border: 1px solid #2a3a4a; border-radius: 6px; transition: all 0.2s; }
  .read-original:hover { background: #1a2a3a; border-color: #4fc3f7; }
  .tags { margin-bottom: 10px; }
  .tag { display: inline-block; background: #1a1a1a; color: #4fc3f7; font-size: 0.72em; padding: 2px 8px; border-radius: 4px; margin-right: 5px; margin-bottom: 4px; }
  .footer { text-align: center; color: #444; font-size: 0.78em; margin-top: 50px; padding-top: 20px; border-top: 1px solid #1a1a1a; }
  .footer a { color: #4fc3f7; text-decoration: none; }
  .toc { background: #111; border-radius: 12px; padding: 20px 24px; margin-bottom: 30px; border: 1px solid #1e1e1e; }
  .toc h3 { font-size: 0.95em; color: #888; margin-bottom: 10px; }
  .toc ul { list-style: none; }
  .toc li { font-size: 0.88em; padding: 4px 0; border-bottom: 1px solid #1a1a1a; }
  .toc li:last-child { border-bottom: none; }
  .toc a { color: #ccc; text-decoration: none; }
  .toc a:hover { color: #4fc3f7; }
  .toc .cat-label { color: #4fc3f7; font-size: 0.8em; margin-left: 8px; }
</style>
</head>
<body>
<div class="container">
<h1>Karpathy RSS å®æ—¶ç²¾é€‰</h1>
<div class="site-desc">åŸºäº Andrej Karpathy æ¨èçš„ 92 ä¸ªé¡¶çº§ç§‘æŠ€åšå®¢ï¼ŒAI ç”Ÿæˆä¸­æ–‡è§£è¯»</div>
<div class="meta">ğŸ“… {{ date }}  |  å…± {{ total }} ç¯‡æ¥è‡ª {{ source_count }} ä¸ªåšå®¢  |  ç”± AI è‡ªåŠ¨ç”Ÿæˆä¸­æ–‡è§£è¯»</div>

<div class="toc">
<h3>ğŸ“‘ ç›®å½•</h3>
<ul>
{% set ns = namespace(idx=0) %}
{% for category, articles in categories.items() %}
{% for a in articles %}
{% set ns.idx = ns.idx + 1 %}
<li><a href="#article-{{ ns.idx }}">{{ ns.idx }}. {{ a.ai_title or a.title }}</a><span class="cat-label">{{ category }}</span></li>
{% endfor %}
{% endfor %}
</ul>
</div>

{% set ns2 = namespace(idx=0) %}
{% for category, articles in categories.items() %}
<div class="category">
<h2>{{ category }}</h2>
{% for a in articles %}
{% set ns2.idx = ns2.idx + 1 %}
<div class="article" id="article-{{ ns2.idx }}">
  <h3>{{ a.ai_title or a.title }}</h3>
  <div class="article-meta">
    <span>ğŸ“ {{ a.source }}{% if a.author %} Â· {{ a.author }}{% endif %}</span>
    <span>ğŸ• {{ a.published.strftime('%Y-%m-%d %H:%M') if a.published else 'è¿‘æœŸ' }}</span>
    {% if a.category and a.category != 'å…¶ä»–' %}<span class="category-tag">{{ a.category }}</span>{% endif %}
  </div>
  {% if a.tags %}<div class="tags">{% for t in a.tags %}<span class="tag">{{ t }}</span>{% endfor %}</div>{% endif %}
  {% if a.ai_detail %}<div class="detail">{{ a.ai_detail }}</div>
  {% elif a.ai_summary %}<div class="detail">{{ a.ai_summary }}</div>{% endif %}
  <a class="read-original" href="{{ a.link }}" target="_blank">ğŸ“– é˜…è¯»è‹±æ–‡åŸæ–‡ â†’</a>
</div>
{% endfor %}
</div>
{% endfor %}

<div class="footer">
  ç”± <a href="https://github.com/" target="_blank">Karpathy RSS Daily Digest</a> è‡ªåŠ¨ç”Ÿæˆ<br>
  æ•°æ®æ¥æº: Andrej Karpathy æ¨èçš„ 92 ä¸ªé¡¶çº§ç§‘æŠ€åšå®¢
</div>
</div>
</body>
</html>
""")

INDEX_TEMPLATE = Template("""\
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Karpathy RSS å®æ—¶ç²¾é€‰</title>
<style>
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Noto Sans SC', sans-serif; background: #0a0a0a; color: #e0e0e0; line-height: 1.8; }
  .container { max-width: 700px; margin: 0 auto; padding: 60px 20px; text-align: center; }
  h1 { font-size: 1.8em; color: #fff; margin-bottom: 10px; }
  .desc { color: #888; margin-bottom: 40px; font-size: 0.95em; }
  .digest-list { text-align: left; }
  .digest-item { background: #111; border-radius: 10px; padding: 18px 22px; margin-bottom: 10px; border: 1px solid #1e1e1e; transition: border-color 0.2s; }
  .digest-item:hover { border-color: #4fc3f7; }
  .digest-item a { color: #e0e0e0; text-decoration: none; font-size: 1.05em; }
  .digest-item a:hover { color: #4fc3f7; }
  .digest-date { color: #666; font-size: 0.82em; margin-top: 4px; }
</style>
</head>
<body>
<div class="container">
<h1>Karpathy RSS å®æ—¶ç²¾é€‰</h1>
<p class="desc">åŸºäº Andrej Karpathy æ¨èçš„ 92 ä¸ªé¡¶çº§ç§‘æŠ€åšå®¢<br>AI è‡ªåŠ¨ç”Ÿæˆä¸­æ–‡è§£è¯»ï¼Œæ¯æ—¥æ›´æ–°</p>
<div class="digest-list">
{% for item in digests %}
<div class="digest-item">
  <a href="{{ item.filename }}">ğŸ“° {{ item.title }}</a>
  <div class="digest-date">{{ item.date }}</div>
</div>
{% endfor %}
{% if not digests %}
<div class="digest-item" style="text-align:center;color:#666;">æš‚æ— å†…å®¹ï¼Œè¯·å…ˆè¿è¡Œ rss_reader.py ç”Ÿæˆç²¾é€‰</div>
{% endif %}
</div>
</div>
</body>
</html>
""")


def generate_html_page(articles: list[Article]) -> str:
    if not articles:
        return ""
    sources = set(a.source for a in articles)
    categories = categorize_articles(articles)
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    context = {
        "date": today,
        "total": len(articles),
        "source_count": len(sources),
        "categories": categories,
    }
    return HTML_TEMPLATE.render(**context)


def save_html_page(content: str) -> Path:
    """ä¿å­˜ HTML åˆ° docs/ ç›®å½•ï¼ˆGitHub Pagesï¼‰"""
    DOCS_DIR.mkdir(parents=True, exist_ok=True)
    today = datetime.now().strftime("%Y-%m-%d")
    filepath = DOCS_DIR / f"{today}.html"
    filepath.write_text(content, encoding="utf-8")
    logger.info(f"ğŸ“„ ç½‘é¡µå·²ä¿å­˜: {filepath}")
    # æ›´æ–° index.html
    _update_index()
    return filepath


def _update_index():
    """æ›´æ–° docs/index.html ç›®å½•é¡µ"""
    DOCS_DIR.mkdir(parents=True, exist_ok=True)
    html_files = sorted(DOCS_DIR.glob("20*.html"), reverse=True)
    digests = []
    for f in html_files[:30]:  # åªå±•ç¤ºæœ€è¿‘30å¤©
        date_str = f.stem  # e.g. "2026-02-25"
        digests.append({
            "filename": f.name,
            "title": f"Karpathy RSS å®æ—¶ç²¾é€‰ - {date_str}",
            "date": date_str,
        })
    index_html = INDEX_TEMPLATE.render(digests=digests)
    (DOCS_DIR / "index.html").write_text(index_html, encoding="utf-8")


def _get_page_url(date_str: str = None) -> str:
    """è·å–å½“å¤©ç½‘é¡µçš„å…¬å¼€ URL"""
    if not GITHUB_PAGES_URL:
        return ""
    if date_str is None:
        date_str = datetime.now().strftime("%Y-%m-%d")
    base = GITHUB_PAGES_URL.rstrip("/")
    return f"{base}/{date_str}.html"


# â”€â”€ Markdown è¾“å‡º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MARKDOWN_TEMPLATE = Template("""\
# Karpathy RSS å®æ—¶ç²¾é€‰

> ğŸ“… {{ date }}  |  å…± {{ total }} ç¯‡æ¥è‡ª {{ source_count }} ä¸ªåšå®¢

---
{% for category, articles in categories.items() %}

## {{ category }}
{% for a in articles %}

### {{ loop.index }}. {{ a.ai_title or a.title }}
- **æ¥æº**: {{ a.source }}{% if a.author %} Â· {{ a.author }}{% endif %}  |  **æ—¶é—´**: {{ a.published.strftime('%Y-%m-%d %H:%M') if a.published else 'è¿‘æœŸ' }}
- **åŸæ–‡**: [{{ a.title }}]({{ a.link }})
{%- if a.tags %}
- **æ ‡ç­¾**: {{ a.tags | join(', ') }}
{%- endif %}
{%- if a.ai_detail %}

{{ a.ai_detail }}
{%- elif a.ai_summary %}

> {{ a.ai_summary }}
{%- endif %}
{% endfor %}

---
{% endfor %}

_ç”± Karpathy RSS Daily Digest è‡ªåŠ¨ç”Ÿæˆ_
""")


def generate_markdown(articles: list[Article]) -> str:
    if not articles:
        return "ä»Šå¤©æš‚æ— æ–°æ–‡ç« ã€‚"
    sources = set(a.source for a in articles)
    categories = categorize_articles(articles)
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    return MARKDOWN_TEMPLATE.render(
        date=today, total=len(articles),
        source_count=len(sources), categories=categories,
    )


def save_markdown(content: str) -> Path:
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    today = datetime.now().strftime("%Y-%m-%d")
    filepath = OUTPUT_DIR / f"digest-{today}.md"
    filepath.write_text(content, encoding="utf-8")
    logger.info(f"ğŸ“„ Markdown å·²ä¿å­˜: {filepath}")
    return filepath


# â”€â”€ æ ¸å¿ƒæµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def fetch_and_process(days: int, since: datetime = None,
                            webhook_url: str = None,
                            sent_db: dict = None,
                            enable_filter: bool = True) -> list[Article]:
    if since is None:
        since = datetime.now(timezone.utc) - timedelta(days=days)

    feeds = parse_opml(FEEDS_FILE)
    articles = await fetch_all_feeds(feeds, since)
    if not articles:
        logger.info("æš‚æ— æ–°æ–‡ç« ")
        return []

    if sent_db is not None:
        before = len(articles)
        articles = filter_new_articles(articles, sent_db)
        skipped = before - len(articles)
        if skipped > 0:
            logger.info(f"ğŸ”„ è·³è¿‡ {skipped} ç¯‡å·²æ¨é€æ–‡ç« ï¼Œå‰©ä½™ {len(articles)} ç¯‡æ–°æ–‡ç« ")
        if not articles:
            logger.info("æ²¡æœ‰æ–°æ–‡ç« éœ€è¦å¤„ç†")
            return []

    await enrich_articles_with_full_content(articles)
    articles = ai_summarize_articles(articles, enable_filter)
    
    if not articles:
        logger.info("ç­›é€‰åæ— ç›¸å…³æ–‡ç« ")
        return []

    # ç”Ÿæˆç½‘é¡µï¼ˆå§‹ç»ˆç”Ÿæˆï¼Œä¾› GitHub Pages ä½¿ç”¨ï¼‰
    html_content = generate_html_page(articles)
    if html_content:
        save_html_page(html_content)

    # æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡
    if webhook_url:
        page_url = _get_page_url()
        await send_to_wecom(webhook_url, articles, page_url)

    if sent_db is not None:
        mark_as_sent(articles, sent_db)
        save_sent_db(sent_db)

    return articles


# â”€â”€ ä¸»é€»è¾‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def run_digest(days: int = 1, fmt: str = "markdown",
                     print_output: bool = True, webhook_url: str = None,
                     enable_filter: bool = True):
    since = datetime.now(timezone.utc) - timedelta(days=days)
    logger.info(f"ğŸš€ å¼€å§‹æŠ“å–ï¼Œæ—¶é—´èŒƒå›´: æœ€è¿‘ {days} å¤© (è‡ª {since.strftime('%Y-%m-%d %H:%M UTC')})")

    sent_db = load_sent_db() if webhook_url else None
    articles = await fetch_and_process(days, since, webhook_url, sent_db, enable_filter)

    if not articles:
        return

    # åŒæ—¶ä¿å­˜ Markdown
    md_content = generate_markdown(articles)
    save_markdown(md_content)

    if print_output:
        print("\n" + "=" * 60)
        if fmt == "html":
            print(f"ç½‘é¡µå·²ç”Ÿæˆ: docs/{datetime.now().strftime('%Y-%m-%d')}.html")
            print(f"Markdown: output/digest-{datetime.now().strftime('%Y-%m-%d')}.md")
        else:
            print(md_content)
        print("=" * 60 + "\n")


async def run_watch(webhook_url: str, interval: int = DEFAULT_WATCH_INTERVAL,
                    days: int = 1, enable_filter: bool = True):
    logger.info(f"ğŸ‘ï¸  å®æ—¶ç›‘æ§æ¨¡å¼å¯åŠ¨")
    logger.info(f"   Webhook: {webhook_url[:50]}...")
    logger.info(f"   è½®è¯¢é—´éš”: æ¯ {interval} åˆ†é’Ÿ")
    logger.info(f"   ç›‘æ§èŒƒå›´: æœ€è¿‘ {days} å¤©çš„æ–°æ–‡ç« ")
    logger.info(f"   å†…å®¹ç­›é€‰: {'å·²å¯ç”¨' if enable_filter else 'å·²ç¦ç”¨'}")
    logger.info(f"   æŒ‰ Ctrl+C åœæ­¢\n")

    try:
        async with httpx.AsyncClient(timeout=httpx.Timeout(10.0)) as client:
            await client.post(webhook_url, json={
                "msgtype": "markdown",
                "markdown": {
                    "content": f"ğŸ¤– **Karpathy RSS ç›‘æ§å·²å¯åŠ¨**\n> æ¯ {interval} åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ–°æ–‡ç« \n> ç›‘æ§ 92 ä¸ªé¡¶çº§ç§‘æŠ€åšå®¢",
                },
            })
    except Exception:
        pass

    round_count = 0
    while True:
        round_count += 1
        logger.info(f"â”€â”€ ç¬¬ {round_count} è½®æ£€æŸ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        try:
            since = datetime.now(timezone.utc) - timedelta(days=days)
            sent_db = load_sent_db()
            articles = await fetch_and_process(days, since, webhook_url, sent_db, enable_filter)
            if articles:
                logger.info(f"âœ… æœ¬è½®æ¨é€äº† {len(articles)} ç¯‡æ–°æ–‡ç« ")
            else:
                logger.info("ğŸ’¤ æœ¬è½®æ— æ–°æ–‡ç« ")
        except Exception as e:
            logger.error(f"âŒ æœ¬è½®æ‰§è¡Œå‡ºé”™: {e}")
        logger.info(f"â³ ç­‰å¾… {interval} åˆ†é’Ÿåè¿›è¡Œä¸‹ä¸€è½®æ£€æŸ¥...\n")
        await asyncio.sleep(interval * 60)


def run_scheduled(days: int, fmt: str, webhook_url: str = None, enable_filter: bool = True):
    import schedule
    import time
    logger.info("â° å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯å¤© 08:00 æ‰§è¡Œ")
    asyncio.run(run_digest(days, fmt, webhook_url=webhook_url, enable_filter=enable_filter))
    schedule.every().day.at("08:00").do(
        lambda: asyncio.run(run_digest(days, fmt, webhook_url=webhook_url, enable_filter=enable_filter))
    )
    while True:
        schedule.run_pending()
        time.sleep(60)


def main():
    parser = argparse.ArgumentParser(
        description="Karpathy RSS å®æ—¶ç²¾é€‰ - 92ä¸ªé¡¶çº§ç§‘æŠ€åšå®¢ AI ä¸­æ–‡è§£è¯» + ä¼ä¸šå¾®ä¿¡æ¨é€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""\
ç¤ºä¾‹:
  python rss_reader.py                                        # æŠ“å–ä»Šå¤©çš„å†…å®¹ï¼ˆé»˜è®¤åªä¿ç•™ç§‘æŠ€/AI/å•†ä¸šç±»ï¼‰
  python rss_reader.py --days 3                               # æŠ“å–æœ€è¿‘3å¤©
  python rss_reader.py --no-filter                            # ç¦ç”¨å†…å®¹ç­›é€‰ï¼Œæ”¶å½•æ‰€æœ‰æ–‡ç« 
  python rss_reader.py --webhook <URL>                        # æŠ“å–å¹¶æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡ç¾¤
  python rss_reader.py --watch --webhook <URL>                # å®æ—¶ç›‘æ§ï¼Œæ–°æ–‡ç« è‡ªåŠ¨æ¨é€
  python rss_reader.py --watch --webhook <URL> --interval 15  # æ¯15åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
  python rss_reader.py --schedule --webhook <URL>             # æ¯å¤©08:00è‡ªåŠ¨æŠ“å–å¹¶æ¨é€
        """,
    )
    parser.add_argument("--days", type=int, default=1, help="æŠ“å–æœ€è¿‘Nå¤©çš„å†…å®¹ (é»˜è®¤: 1)")
    parser.add_argument("--output", choices=["markdown", "html"], default="html", help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: html)")
    parser.add_argument("--webhook", type=str, default=None, help="ä¼ä¸šå¾®ä¿¡ç¾¤ Webhook URL (æˆ–è®¾ç½® WECOM_WEBHOOK_URL ç¯å¢ƒå˜é‡)")
    parser.add_argument("--watch", action="store_true", help="å®æ—¶ç›‘æ§æ¨¡å¼")
    parser.add_argument("--interval", type=int, default=DEFAULT_WATCH_INTERVAL, help=f"è½®è¯¢é—´éš”åˆ†é’Ÿæ•° (é»˜è®¤: {DEFAULT_WATCH_INTERVAL})")
    parser.add_argument("--schedule", action="store_true", help="å®šæ—¶ä»»åŠ¡æ¨¡å¼ï¼ˆæ¯å¤©08:00ï¼‰")
    parser.add_argument("--no-filter", action="store_true", help="ç¦ç”¨å†…å®¹ç­›é€‰ï¼ˆæ”¶å½•æ‰€æœ‰ç±»åˆ«æ–‡ç« ï¼‰")
    args = parser.parse_args()

    # é»˜è®¤å¯ç”¨å†…å®¹ç­›é€‰ï¼Œé™¤éæŒ‡å®š --no-filter
    enable_filter = ENABLE_CONTENT_FILTER and not args.no_filter
    
    # æ”¯æŒ webhook ä»ç¯å¢ƒå˜é‡è¯»å–
    webhook_url = args.webhook or os.environ.get("WECOM_WEBHOOK_URL")

    if args.watch and not webhook_url:
        parser.error("--watch æ¨¡å¼éœ€è¦é…åˆ --webhook ä½¿ç”¨æˆ–è®¾ç½® WECOM_WEBHOOK_URL ç¯å¢ƒå˜é‡")

    if args.watch:
        asyncio.run(run_watch(webhook_url, args.interval, args.days, enable_filter))
    elif args.schedule:
        run_scheduled(args.days, args.output, webhook_url, enable_filter)
    else:
        asyncio.run(run_digest(args.days, args.output, webhook_url=webhook_url, enable_filter=enable_filter))


if __name__ == "__main__":
    main()
